import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers,optimizers,datasets
#datasets是TensorFlow Keras API的一部分，它提供了对多个标准数据集的访问，包括MNIST手写数字数据集。
#datasets 模块中有一个 mnist 对象，该对象具有 load_data() 方法
import os
import numpy as np
import matplotlib.pyplot as plt

import sys






version = tf.__version__       # 获取TensorFlow的版本号
cpu = tf.config.list_physical_devices('CPU')    # 列出系统中所有物理CPU设备
gpu = tf.config.list_physical_devices('GPU')    # 列出系统中所有物理GPU设备
print("tensorflow version:",version,"\nCPU：",cpu,"\nGPU：",gpu)  # 打印TensorFlow版本，以及CPU和GPU设备的信息
























# 导入 TensorFlow 库
import tensorflow as tf

# 定义一个普通的 Python 浮点数变量 a，并赋值为 2.1
a = 2.1

"""使用 TensorFlow 的 tf.constant 方法创建一个值为 2.1 的常量张量 b
# tf.constant(value, dtype=None, shape=None, name='Const', verify_shape=False)
# - value: 要创建张量的值，可以是标量、列表、NumPy 数组等
# - dtype: 指定张量的数据类型，如果未指定，将根据 value 的类型自动推断
# - shape: 指定张量的形状，如果未指定，且 value 是一个序列，将根据序列的长度推断形状
# - name: 为创建的张量指定一个名称，有助于在 TensorFlow 图中识别
# - verify_shape: 如果设置为 True，将检查 value 的形状是否与指定的形状匹配，不匹配将抛出错误"""
b = tf.constant(2.1)

# 打印 a 和 b 是否为 TensorFlow 张量的布尔值。tf.is_tensor(x) 用于判断 x 是否为 TensorFlow 张量
print(tf.is_tensor(a), tf.is_tensor(b))

# 打印变量 a 的值
print(a)

# 打印变量 b 的值，由于 b 是一个 TensorFlow 张量，打印输出将显示张量的详细信息
# 包括张量的名称、形状、数据类型等
print(b)
# b的shape=(),这表示b是一个0阶也就是0维的数值，是一个标量，只有大小。





a = tf.constant([1,2,3]) #这里传入的是一个列表 [1, 2, 3]，它将被转换为一个 TensorFlow 张量
print(a) #打印结果表明 a 是一个形状为 (3,)的 TensorFlow 张量（即长度为 3 的一维张量）





a= np.array([[1,2,3],[4,5,6],[7,8,9]])
# 使用 'tf.constant' 函数创建一个TensorFlow张量
# 这里，'a' 被转换为一个TensorFlow张量b，且b是常量，其值不会改变
b = tf.constant(a)
print(b)






a= np.array([[[1,2,3],[4,5,6],[7,8,9]],[[1,2,3],[4,5,6],[7,8,9]],[[1,2,3],[4,5,6],[7,8,9]]])
b = tf.constant(a)
print(b)





a = tf.constant('Hello,TensorFlow!')
print(a)





a = tf.constant(np.pi,dtype=tf.float32)
b = tf.constant(np.pi,dtype=tf.float64)
print(a,b)











a=80
b= tf.constant(80)
print(a)
print(b)

print(b.ndim)  # 打印张量b的维度数（ndim） 输出: 0 表示`b`是一个标量（scalar），没有维度
print(b.shape) # 打印张量b的形状（shape） 输出: () 表示`b`是一个单值张量，没有维度
print(b.numpy()) # 使用b.numpy()获取张量b的NumPy表示
print(b.device) # 打印张量b所在的设备（device）
print(tf.rank(b)) # 使用tf.rank获取张量b的秩（rank）

# 在TensorFlow中，b.ndim和tf.rank(b)通常返回相同的结果














a = tf.range(1,20,2)
print(a.ndim)
print(a.shape)
print(a.numpy())
print(a)










a = tf.random.normal([50],80,10,tf.float16) 
#创建一个包含 50 个元素的一维张量
# mean 参数80，表示生成的随机数的均值。在这个例子中，生成的正态分布的均值是 80
# stddev参数10，表示生成的随机数的标准差。在这个例子中，生成的正态分布的标准差是 10
# 标准差是方差的平方根。方差（Variance）是各数据与其平均值差的平方的平均值。
print(a)
"""
一个班级的考试成绩均值为80，标准差为10的正态分布表示
大约68%的学生分数在70到90分之间（80±10），95%的学生分数在60到100分之间（80 ± 20），
几乎所有学生（99.7%）的分数在50 到110分之间（80 ± 60）"""











a = tf.random.uniform([20,4],50,100,tf.dtypes.float16)
print(a.numpy())





a = tf.random.uniform([12,4],60,100,tf.dtypes.float16)
print(a.numpy())





a = tf.zeros([12,3])
print(a)


a = tf.ones([12,3])
print(a)


a = tf.fill([12,3],-1) #创建一个12行3列，值为-1的2维张量
print(a)








a= tf.fill([12,3,10],0) #12层，每层3行10列，每个位置初始值为0的矩阵
#也可以理解成12行3列，每个位置是一个包含10个数值的一维向量。逻辑客观，解释主观，解释需要联系上下文。
print(a)
b=tf.fill([12,3,20,5],1)#12层,每层3行4列，每个位置5个数值，初始值为1的矩阵
print(b)
#tf创建的是张量，可以用


a = tf.random.uniform([12,3,10],60,100,tf.int32)
print(a)











# 加载 MNIST 数据集，数据集被分为训练集和测试集，每个集合中包含图像和对应的标签。
(x_train,y_train),(x_test,y_test) = datasets.mnist.load_data()
#x_train, y_train：这是训练数据部分，x_train 包含图像数据，y_train 包含每张图像对应的标签。
#x_test, y_test：这是测试数据部分，x_test 包含图像数据，y_test 包含每张图像对应的标签。
print(x_train.shape)
print(y_train.shape)
print(x_train[0])
print(y_train[0])











print(y_train)
y_train = tf.convert_to_tensor(y_train,dtype=tf.int32)
y_train = tf.one_hot(y_train,depth=10)
print(y_train)








fig,ax = plt.subplots(nrows=2,ncols=10,sharex=True,sharey=True,)
ax = ax.flatten()
for i in range(20):
    img = x_train[i]
    ax[i].imshow(img,cmap='Greys',interpolation = 'nearest')
ax[0].set_xticks([])
ax[0].set_yticks([])
plt.tight_layout()
plt.show()














a = tf.constant([1,2,3,4,5,6,7,8,9,10])
print(a[0].numpy())
print(a[0])
print(a[-1].numpy())
print(a[-1])


print(a[:].numpy())


print(a[1:4:1].numpy())


print(a[:4].numpy())


print(a[4:].numpy())


print(a[::2].numpy())


print(a[::-1].numpy())


print(a[3:7:1].numpy())





(x_train,y_train),(x_test,y_test) = datasets.mnist.load_data()


print(x_train.shape)
sample = x_train[4:8]
print(sample.shape)


fig,ax = plt.subplots(nrows=1,ncols=4,sharex=True,sharey=True,)
ax = ax.flatten()
for i in range(4):
    img = sample[i]
    ax[i].imshow(img,cmap='Greys',interpolation = 'nearest')
ax[0].set_xticks([])
ax[0].set_yticks([])
plt.tight_layout()
plt.show()








sample = x_train[1:5,0:28:2,0:28:2]
print(sample.shape)


fig,ax = plt.subplots(nrows=1,ncols=4,sharex=True,sharey=True,)
ax = ax.flatten()
for i in range(4):
    img = sample[i]
    ax[i].imshow(img,cmap='Greys',interpolation = 'nearest')
ax[0].set_xticks([])
ax[0].set_yticks([])
plt.tight_layout()
plt.show()





sample = x_train[0:4]
print(sample.shape)


a = tf.image.grayscale_to_rgb(tf.reshape(sample,(4,28,28,1)))


print(a.shape)


fig,ax = plt.subplots(nrows=1,ncols=4,sharex=True,sharey=True,)
ax = ax.flatten()
for i in range(4):
    img = a[i]
    ax[i].imshow(img)
ax[0].set_xticks([])
ax[0].set_yticks([])
plt.tight_layout()
plt.show()











(x_train,y_train),(x_test,y_test) = datasets.mnist.load_data()
sample = x_train[4:8]
print(sample.shape)





a = tf.reshape(sample,[4,784])
print(a.shape)


a = tf.reshape(sample,[4,-1])
print(a.shape)


a = tf.reshape(sample,[4,2,-1])
print(a.shape)


a = tf.reshape(sample,[4,28,28,1])
print(a.shape)


a = tf.reshape(sample,[4,-1,1])
print(a.shape)





a = tf.random.uniform([28,28],maxval=10,dtype=tf.int32)
print(a.shape)
a = tf.expand_dims(a,axis=0)
print(a.shape)





a = tf.random.uniform([4,28,28,3],maxval=10,dtype=tf.int32)
print(a.shape)
a1 = tf.expand_dims(a,axis=0)
print(a1.shape)
a2 = tf.expand_dims(a,axis=1)
print(a2.shape)
a3 = tf.expand_dims(a,axis=2)
print(a3.shape)
a4 = tf.expand_dims(a,axis=3)
print(a4.shape)
a5 = tf.expand_dims(a,axis=4)
print(a5.shape)






a = tf.random.uniform([1,28,28,1],maxval=10,dtype=tf.int32)
print(a.shape)
a= tf.squeeze(a,axis=0)
print(a.shape)








a = tf.constant([[1,2,3],[4,5,6]])
print(a.shape)
print(a.numpy())


b= tf.transpose(a,perm=[1,0])
print(b.shape)
print(b.numpy())





a = tf.constant([[[ 1,  2,  3],
                  [ 4,  5,  6]],
                 [[ 7,  8,  9],
                  [10, 11, 12]]])
print(a.shape)
print(a.numpy())


b = tf.transpose(a,perm=[0,2,1])
print(b.shape)
print(b.numpy())





a = tf.random.normal([10,3,28,28])
print(a.shape)
b = tf.transpose(a,[0,2,3,1])
print(b.shape)









a = tf.constant([1,2,3])
a = tf.expand_dims(a,axis=0)
print(a.shape)


a= tf.tile(a,[4,1])
print(a.shape)
print(a.numpy())


b = tf.constant([1,2,3])
b = tf.broadcast_to(b,[4,3])
print(b)





a = tf.random.normal([28,1])
a = tf.broadcast_to(a,[4,28,28,3])
print(a.shape)

















a = tf.random.normal([5,5])
b = tf.random.normal([5,5])
c = tf.concat([a,b],axis=0)
print(a.shape,b.shape,c.shape)





a = tf.random.normal([5,5])
b = tf.random.normal([5,3])
c = tf.concat([a,b],axis=1)
print(a.shape,b.shape,c.shape)





a = tf.random.normal([4,35,8])
b = tf.random.normal([6,35,8])
c = tf.concat([a,b],axis=0)
print(a.shape,b.shape,c.shape)








a = tf.random.normal([10,35,8])
r = tf.split(a,[4,6],axis=0)



print(r[0].shape)
print(r[1].shape)








a = tf.constant([4,5,6,3,2,4])
print(a)
print(tf.reduce_max(a))
print(tf.math.reduce_max(a))


a = tf.random.normal([3,5])
print(a)
print(tf.math.reduce_max(a,axis=0))





a = tf.constant([4,5,6,3,2,4])
print(a)
print(tf.math.reduce_mean(a))


a = tf.random.normal([3,5])
print(a)
print(tf.math.reduce_mean(a,axis=1))





a = tf.constant([4,5,6,3,2,4])
print(a)
print(tf.math.reduce_sum(a))


a = tf.random.normal([3,5])
print(a)
print(tf.math.reduce_sum(a,))
print(tf.math.reduce_sum(a,axis=0))
print(tf.math.reduce_sum(a,axis=1))





a = tf.constant([2,4])
b = tf.constant(2)
print(tf.math.equal(a,b))


a = tf.constant([2,4])
b = tf.constant([2,4])
print(tf.math.equal(a,b))


a = tf.constant([1,2,3,4])
b = tf.constant([[1],[2],[3],[4]])
c = tf.math.equal(a,b)
print(c)


a = tf.constant([4,5,6])
b = tf.constant([5])
c = tf.math.greater(a,b)
print(c)


a = tf.constant([1,2,4,5,6])
b = tf.constant([4,5,6,2,2])
c = tf.math.less(a,b)
print(c)


a = tf.random.normal([3,4])
b = tf.random.normal([3,4])
c = tf.math.less(a,b)
print(c)





o = tf.random.normal([100,10])
o = tf.nn.softmax(o,axis=1)


pred = tf.argmax(o,axis=1)
print(pred)


y = tf.random.uniform([100],dtype=tf.int64,maxval=10)
print(y)


c = tf.equal(pred,y)
print(c)


c = tf.cast(c,dtype=tf.float32)
print(c)


acc = tf.reduce_sum(c)
print(acc)








a = tf.random.shuffle(tf.range(6))
print(a)
a = tf.sort(a)
print(a)
a = tf.sort(a,direction='DESCENDING')
print(a)





values = [1, 10, 26.9, 2.8, 166.32, 62.3]
sort_order = tf.argsort(values)
sort_order.numpy()


mat = [[30,20,10],
       [20,10,30],
       [10,30,20]]
indices = tf.argsort(mat)
indices.numpy()





result = tf.math.top_k([1, 2, 98, 1, 1, 99, 3, 1, 3, 96, 4, 1], k=3)
result.values.numpy()

result.indices.numpy()


input = tf.random.normal(shape=(3,4,5,6))
k = 2
values, indices  = tf.math.top_k(input, k=k)
values.shape.as_list()
values.shape == indices.shape == input.shape[:-1] + [k]





a = tf.constant([8,9,7,5,4,2])
indices=tf.constant([2,1,4,5])
a = tf.gather(a,indices)
print(a)





a = tf.constant([[2, 5, 6, 3], [5, 4, 4, 9], [7, 1, 7, 5]])
indices = tf.constant([1, 2, 0])
a = tf.gather(a, indices, axis=0)
print(a)


a = tf.constant([[2, 5, 6, 3], [5, 4, 4, 9], [7, 1, 7, 5]])
indices = tf.constant([1, 2, 0])
a = tf.gather(a, indices, axis=1)
print(a)


t = tf.constant([[[1, 1, 1], [2, 2, 2]],
                 [[3, 3, 3], [4, 4, 4]],
                 [[5, 5, 5], [6, 6, 6]]])
tf.slice(t, [1, 0, 0], [1, 1, 3])  # [[[3, 3, 3]]]
tf.slice(t, [1, 0, 0], [1, 2, 3])  # [[[3, 3, 3],
                                   #   [4, 4, 4]]]
tf.slice(t, [1, 0, 0], [2, 1, 3])  # [[[3, 3, 3]],
                                   #  [[5, 5, 5]]]


t = tf.constant([[[1, 1, 1], [2, 2, 2]],
                 [[3, 3, 3], [4, 4, 4]],
                 [[5, 5, 5], [6, 6, 6]]])
tf.strided_slice(t, [1, 0, 0], [2, 1, 3], [1, 1, 1])  # [[[3, 3, 3]]]
tf.strided_slice(t, [1, 0, 0], [2, 2, 3], [1, 1, 1])  # [[[3, 3, 3],
                                                      #   [4, 4, 4]]]
tf.strided_slice(t, [1, -1, 0], [2, -3, 3], [1, -1, 1])  # [[[4, 4, 4],
                                                         #   [3, 3, 3]]]



